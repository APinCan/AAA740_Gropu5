{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f5c409",
   "metadata": {},
   "source": [
    "# imported packages\n",
    "\n",
    "- to make running easier, dependencies on tensorflow are replaced with pytorch\n",
    "- basic packages\n",
    "- data processing pacakges\n",
    "- torch\n",
    "- custom imports\n",
    "    - models_search: this defines search space(building blocks), RNN controller and GAN design\n",
    "    - funcitons.py: contains functions for training and validation of GAN and RNN controller\n",
    "    - utils:\n",
    "        - inception_score_torch.py: pytorch implementation of inception score\n",
    "            - link: https://github.com/sbarratt/inception-score-pytorch.git\n",
    "            - this is not perfect; inception score on original cifar-10 should be 11.24 ± 0.20 on 50k train images\n",
    "            - however, this implementation yeilds about 10.5\n",
    "        - fid_score_torch.py: pytorch implementation of Frechet inception distance score\n",
    "            - link: https://github.com/mseitzer/pytorch-fid.git\n",
    "        - utils.py: nothing changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae330f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python packages -- no need to install\n",
    "import os\n",
    "import time\n",
    "import datasets\n",
    "import datetime\n",
    "\n",
    "# data processing packages\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imsave\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# utility packages\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom imports\n",
    "## hyperparameters\n",
    "import cfg\n",
    "\n",
    "## in models\n",
    "import models_search\n",
    "\n",
    "## import training functions\n",
    "from functions import get_topk_arch_hidden, train_controller, train_shared\n",
    "\n",
    "## from utils: computing inception score and fid score depends on tensorflow --> change to pytorch implementation (taegun)\n",
    "from utils import inception_score_torch as ist\n",
    "from utils import fid_score_torch as fst\n",
    "from utils.utils import create_logger, RunningStats, save_checkpoint, set_log_dir\n",
    "\n",
    "\n",
    "# author's setting\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892e1b0",
   "metadata": {},
   "source": [
    "# Prepare NAS\n",
    "\n",
    "- from cfg.py: configurations (hyperparameters)\n",
    "- from exps:\n",
    "    - from autogan_search.sh: load parameters from this shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575d64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = cfg.parse_args()\n",
    "torch.cuda.manual_seed(args.random_seed)\n",
    "\n",
    "# this should be set\n",
    "args.exp_name = 'search_{}'.format(datetime.datetime.now())\n",
    "\n",
    "# parameters from exps/autogan_search.sh\n",
    "args.gen_bs = 128\n",
    "args.dis_bs = 64\n",
    "dataset = 'cifar10'\n",
    "args.bottom_width = 4\n",
    "img_size = 32\n",
    "args.max_iter = 50000\n",
    "args.gen_model = 'shared_gan'\n",
    "args.dis_model = 'shared_gan'\n",
    "args.latent_dim = 128\n",
    "args.gf_dim = 256\n",
    "args.df_dim = 128\n",
    "args.g_spectral_norm = False\n",
    "args.d_spectral_norm = True\n",
    "args.g_lr = 0.0002\n",
    "args.d_lr = 0.0002\n",
    "args.beta1 = 0.0\n",
    "args.beta2 = 0.9\n",
    "args.init_type = 'xavier_uniform'\n",
    "args.n_critic = 5\n",
    "args.val_freq = 20\n",
    "#args.arch = '1 0 1 1 1 0 0 1 1 1 0 1 0 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baefaad2",
   "metadata": {},
   "source": [
    "# Before Search.py\n",
    "\n",
    "- used classes & functions\n",
    "    - record_val function is used to record values to tensorboard (by taegun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5eb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrowCtrler(object):\n",
    "    def __init__(self, grow_step1, grow_step2):\n",
    "        self.grow_step1 = grow_step1\n",
    "        self.grow_step2 = grow_step2\n",
    "\n",
    "    def cur_stage(self, search_iter):\n",
    "        \"\"\"\n",
    "        Return current stage.\n",
    "        :param epoch: current epoch.\n",
    "        :return: current stage\n",
    "        \"\"\"\n",
    "        if search_iter < self.grow_step1:\n",
    "            return 0\n",
    "        elif self.grow_step1 <= search_iter < self.grow_step2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "\n",
    "def create_ctrler(args, cur_stage, weights_init):\n",
    "    controller = eval(\"models_search.\" + args.controller + \".Controller\")(\n",
    "        args=args, cur_stage=cur_stage\n",
    "    ).cuda()\n",
    "    controller.apply(weights_init)\n",
    "    ctrl_optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, controller.parameters()),\n",
    "        args.ctrl_lr,\n",
    "        (args.beta1, args.beta2),\n",
    "    )\n",
    "    return controller, ctrl_optimizer\n",
    "\n",
    "\n",
    "def create_shared_gan(args, weights_init):\n",
    "    gen_net = eval(\"models_search.\" + args.gen_model + \".Generator\")(args=args).cuda()\n",
    "    dis_net = eval(\"models_search.\" + args.dis_model + \".Discriminator\")(\n",
    "        args=args\n",
    "    ).cuda()\n",
    "    gen_net.apply(weights_init)\n",
    "    dis_net.apply(weights_init)\n",
    "    gen_optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, gen_net.parameters()),\n",
    "        args.g_lr,\n",
    "        (args.beta1, args.beta2),\n",
    "    )\n",
    "    dis_optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, dis_net.parameters()),\n",
    "        args.d_lr,\n",
    "        (args.beta1, args.beta2),\n",
    "    )\n",
    "    return gen_net, dis_net, gen_optimizer, dis_optimizer\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        if args.init_type == \"normal\":\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif args.init_type == \"orth\":\n",
    "            nn.init.orthogonal_(m.weight.data)\n",
    "        elif args.init_type == \"xavier_uniform\":\n",
    "            nn.init.xavier_uniform(m.weight.data, 1.0)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"{} unknown inital type\".format(args.init_type)\n",
    "            )\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "\n",
    "def record_val(args, fixed_z, gen_net: nn.Module, writer, step, clean_dir=False):\n",
    "    fid_stat = './data/fid_stats_cifar10_train.npz'\n",
    "    print('==== validation runs!!! ====')\n",
    "\n",
    "    # eval mode\n",
    "    gen_net = gen_net.eval()\n",
    "\n",
    "    # generate images\n",
    "    sample_imgs = gen_net(fixed_z)\n",
    "    img_grid = torchvision.utils.make_grid(sample_imgs, nrow=5, normalize=True, scale_each=True)\n",
    "\n",
    "    # get fid and inception score\n",
    "    fid_buffer_dir = os.path.join(args.path_helper[\"sample_path\"], \"fid_buffer\")\n",
    "    os.makedirs(fid_buffer_dir, exist_ok=True)\n",
    "\n",
    "    eval_iter = args.num_eval_imgs // args.eval_batch_size\n",
    "    img_list = list()\n",
    "    for iter_idx in tqdm(range(eval_iter), desc=\"sample images\"):\n",
    "        z = torch.cuda.FloatTensor(\n",
    "            np.random.normal(0, 1, (args.eval_batch_size, args.latent_dim))\n",
    "        )\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = (\n",
    "            gen_net(z)\n",
    "            .mul_(127.5)\n",
    "            .add_(127.5)\n",
    "            .clamp_(0.0, 255.0)\n",
    "            .permute(0, 2, 3, 1)\n",
    "            .to(\"cpu\", torch.uint8)\n",
    "            .numpy()\n",
    "        )\n",
    "        for img_idx, img in enumerate(gen_imgs):\n",
    "            file_name = os.path.join(fid_buffer_dir, f\"iter{iter_idx}_b{img_idx}.png\")\n",
    "            imsave(file_name, img)\n",
    "        img_list.extend(list(gen_imgs))\n",
    "\n",
    "    # get inception score\n",
    "    x = np.asarray(img_list)\n",
    "    print(x.shape)\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Scale(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    xs = []\n",
    "    for d in x:\n",
    "        d = Image.fromarray(d)\n",
    "        d = trans(d)\n",
    "        xs.append(torch.unsqueeze(d, 0))\n",
    "\n",
    "    b = torch.Tensor(10000, 3, 32, 32)\n",
    "    torch.cat(xs, out=b)\n",
    "    mean, std = ist.get_IS(b)\n",
    "\n",
    "    # get fid score\n",
    "    fid_score = fst.get_fid(fid_buffer_dir, fid_stat)\n",
    "\n",
    "    if clean_dir:\n",
    "        os.system(\"rm -r {}\".format(fid_buffer_dir))\n",
    "    else:\n",
    "        logger.info(f\"=> sampled images are saved to {fid_buffer_dir}\")\n",
    "\n",
    "    writer.add_image(\"sampled_images\", img_grid)\n",
    "    writer.add_scalar(\"Inception_score/mean\", mean, step)\n",
    "    writer.add_scalar(\"Inception_score/std\", std, step)\n",
    "    writer.add_scalar(\"FID_score\", fid_score, step)\n",
    "    \n",
    "    return mean, fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623124d",
   "metadata": {},
   "source": [
    "# Search.py\n",
    "\n",
    "- below cell have contents originally contained in search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4f9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-08bd3f029627>:61: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight.data, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> start new search process\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] 파일 이름, 디렉터리 이름 또는 볼륨 레이블 구문이 잘못되었습니다: 'logs\\\\search_2021-10-20 10:58:17.650729_2021_10_20_10_59_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-becf79cbe56c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# create new log dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_helper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_log_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"logs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_helper\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"log_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprev_archs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AAA740_project\\AutoGAN-taegun\\utils\\utils.py\u001b[0m in \u001b[0;36mset_log_dir\u001b[1;34m(root_dir, exp_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtimestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y_%m_%d_%H_%M_%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mpath_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prefix\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\scratch\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] 파일 이름, 디렉터리 이름 또는 볼륨 레이블 구문이 잘못되었습니다: 'logs\\\\search_2021-10-20 10:58:17.650729_2021_10_20_10_59_00'"
     ]
    }
   ],
   "source": [
    "# set writer for tensorboard\n",
    "writer = SummaryWriter('./runs/')\n",
    "\n",
    "# define nets\n",
    "gen_net, dis_net, gen_optimizer, dis_optimizer = create_shared_gan(args, weights_init)\n",
    "\n",
    "# set grow controller\n",
    "grow_ctrler = GrowCtrler(args.grow_step1, args.grow_step2)\n",
    "\n",
    "# initial\n",
    "start_search_iter = 0\n",
    "\n",
    "# set writer\n",
    "if args.load_path:\n",
    "    print(f\"=> resuming from {args.load_path}\")\n",
    "    assert os.path.exists(args.load_path)\n",
    "    checkpoint_file = os.path.join(args.load_path, \"Model\", \"checkpoint.pth\")\n",
    "    assert os.path.exists(checkpoint_file)\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    # set controller && its optimizer\n",
    "    cur_stage = checkpoint[\"cur_stage\"]\n",
    "    controller, ctrl_optimizer = create_ctrler(args, cur_stage, weights_init)\n",
    "\n",
    "    start_search_iter = checkpoint[\"search_iter\"]\n",
    "    gen_net.load_state_dict(checkpoint[\"gen_state_dict\"])\n",
    "    dis_net.load_state_dict(checkpoint[\"dis_state_dict\"])\n",
    "    controller.load_state_dict(checkpoint[\"ctrl_state_dict\"])\n",
    "    gen_optimizer.load_state_dict(checkpoint[\"gen_optimizer\"])\n",
    "    dis_optimizer.load_state_dict(checkpoint[\"dis_optimizer\"])\n",
    "    ctrl_optimizer.load_state_dict(checkpoint[\"ctrl_optimizer\"])\n",
    "    prev_archs = checkpoint[\"prev_archs\"]\n",
    "    prev_hiddens = checkpoint[\"prev_hiddens\"]\n",
    "    args.path_helper = checkpoint[\"path_helper\"]    \n",
    "    logger.info(f\"=> loaded checkpoint {checkpoint_file} (search iteration {start_search_iter})\")\n",
    "    logger = create_logger(args.path_helper[\"log_path\"])\n",
    "    \n",
    "else:\n",
    "    print(f\"=> start new search process\")\n",
    "    # create new log dir\n",
    "    assert args.exp_name\n",
    "    args.path_helper = set_log_dir(\"logs\", args.exp_name)\n",
    "    logger = create_logger(args.path_helper[\"log_path\"])\n",
    "    prev_archs = None\n",
    "    prev_hiddens = None\n",
    "\n",
    "    # set controller && its optimizer\n",
    "    cur_stage = 0\n",
    "    controller, ctrl_optimizer = create_ctrler(args, cur_stage, weights_init)\n",
    "\n",
    "# set up data_loader\n",
    "dataset = datasets.ImageDataset(args, 2 ** (cur_stage + 3))\n",
    "train_loader = dataset.train\n",
    "\n",
    "logger.info(args)\n",
    "writer_dict = {\n",
    "    \"writer\": SummaryWriter(args.path_helper[\"log_path\"]),\n",
    "    \"controller_steps\": start_search_iter * args.ctrl_step,\n",
    "}\n",
    "\n",
    "g_loss_history = RunningStats(args.dynamic_reset_window)\n",
    "d_loss_history = RunningStats(args.dynamic_reset_window)\n",
    "\n",
    "fixed_z = torch.cuda.FloatTensor(np.random.normal(0, 1, (25, args.latent_dim)))\n",
    "\n",
    "# train loop\n",
    "for search_iter in tqdm(range(int(start_search_iter), int(args.max_search_iter)), desc=\"search progress\"):\n",
    "    \n",
    "    logger.info(f\"<start search iteration {search_iter}>\")\n",
    "    if search_iter == args.grow_step1 or search_iter == args.grow_step2:\n",
    "\n",
    "        # save\n",
    "        cur_stage = grow_ctrler.cur_stage(search_iter)\n",
    "        logger.info(f\"=> grow to stage {cur_stage}\")\n",
    "        prev_archs, prev_hiddens = get_topk_arch_hidden(\n",
    "            args, controller, gen_net, prev_archs, prev_hiddens\n",
    "        )\n",
    "        \n",
    "        print(prev_archs)\n",
    "        \n",
    "        # grow section\n",
    "        del controller\n",
    "        del ctrl_optimizer\n",
    "        controller, ctrl_optimizer = create_ctrler(args, cur_stage, weights_init)\n",
    "\n",
    "        dataset = datasets.ImageDataset(args, 2 ** (cur_stage + 3))\n",
    "        train_loader = dataset.train\n",
    "\n",
    "    dynamic_reset = train_shared(\n",
    "        args,\n",
    "        gen_net,\n",
    "        dis_net,\n",
    "        g_loss_history,\n",
    "        d_loss_history,\n",
    "        controller,\n",
    "        gen_optimizer,\n",
    "        dis_optimizer,\n",
    "        train_loader,\n",
    "        prev_hiddens=prev_hiddens,\n",
    "        prev_archs=prev_archs,\n",
    "    )\n",
    "    \n",
    "    train_controller(\n",
    "        args,\n",
    "        controller,\n",
    "        ctrl_optimizer,\n",
    "        gen_net,\n",
    "        prev_hiddens,\n",
    "        prev_archs,\n",
    "        writer_dict,\n",
    "    )\n",
    "    \n",
    "    print('record to tensorboard')\n",
    "    Iscore, Fscore = record_val(args, fixed_z, gen_net, writer, search_iter)\n",
    "    gen_net.train()\n",
    "    \n",
    "    if dynamic_reset:\n",
    "        logger.info(\"re-initialize share GAN\")\n",
    "        del gen_net, dis_net, gen_optimizer, dis_optimizer\n",
    "        gen_net, dis_net, gen_optimizer, dis_optimizer = create_shared_gan(args, weights_init)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"cur_stage\": cur_stage,\n",
    "            \"search_iter\": search_iter + 1,\n",
    "            \"gen_model\": args.gen_model,\n",
    "            \"dis_model\": args.dis_model,\n",
    "            \"controller\": args.controller,\n",
    "            \"gen_state_dict\": gen_net.state_dict(),\n",
    "            \"dis_state_dict\": dis_net.state_dict(),\n",
    "            \"ctrl_state_dict\": controller.state_dict(),\n",
    "            \"gen_optimizer\": gen_optimizer.state_dict(),\n",
    "            \"dis_optimizer\": dis_optimizer.state_dict(),\n",
    "            \"ctrl_optimizer\": ctrl_optimizer.state_dict(),\n",
    "            \"prev_archs\": prev_archs,\n",
    "            \"prev_hiddens\": prev_hiddens,\n",
    "            \"path_helper\": args.path_helper,\n",
    "        },\n",
    "        False,\n",
    "        args.path_helper[\"ckpt_path\"],\n",
    "    )\n",
    "    \n",
    "    search_iter += 1\n",
    "\n",
    "final_archs, _ = get_topk_arch_hidden(args, controller, gen_net, prev_archs, prev_hiddens)\n",
    "logger.info(f\"discovered archs: {final_archs}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7bd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
